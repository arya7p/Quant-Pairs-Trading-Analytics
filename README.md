# Quant Pairs Trading Analytics System

**End-to-end quantitative analytics platform for statistical arbitrage and pairs trading**

Real-time market data ingestion, processing, and visualization with statistical analytics for identifying mean-reversion opportunities.

---

## Overview

This system demonstrates a complete quantitative trading analytics pipeline:

```
Binance WebSocket â†’ Ingestion â†’ Storage â†’ Resampling â†’ Analytics â†’ Dashboard â†’ Alerts
```

### Key Features

âœ… **Live Data Collection**: Real-time tick data from Binance Futures WebSocket  
âœ… **Multi-Timeframe Resampling**: 1s, 1m, 5m, 15m OHLCV bars  
âœ… **Pairs Analytics**: Hedge ratio, spread, z-score, correlation  
âœ… **Statistical Tests**: Augmented Dickey-Fuller test for stationarity  
âœ… **Interactive Dashboard**: Streamlit-based UI with Plotly charts  
âœ… **Alert System**: User-defined thresholds for trading signals  
âœ… **Data Export**: CSV downloads for further analysis  

---

## Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Binance Futures â”‚
â”‚   WebSocket API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Live Ticks
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion    â”‚ â† BinanceWSCollector (async)
â”‚  â€¢ Tick Buffer     â”‚
â”‚  â€¢ Normalization   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage Layer     â”‚ â† SQLite Database
â”‚  â€¢ Ticks Table     â”‚
â”‚  â€¢ Resampled OHLCV â”‚
â”‚  â€¢ Alerts Log      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Resampling Engine â”‚ â† Pandas resample
â”‚  â€¢ 1s, 1m, 5m, etc â”‚
â”‚  â€¢ OHLCV bars      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics Engine   â”‚ â† Statsmodels + NumPy
â”‚  â€¢ OLS Regression  â”‚
â”‚  â€¢ Spread & Z-Scoreâ”‚
â”‚  â€¢ Correlation     â”‚
â”‚  â€¢ ADF Test        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit Dashboardâ”‚ â† Plotly visualization
â”‚  â€¢ Real-time chartsâ”‚
â”‚  â€¢ Controls        â”‚
â”‚  â€¢ Alerts          â”‚
â”‚  â€¢ Export          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

| Component | Technology | Purpose | Scaling Considerations |
|-----------|-----------|---------|----------------------|
| **Ingestion** | `websockets`, `asyncio` | Collect live tick data | Replace with Kafka for distributed collection |
| **Storage** | SQLite | Persist ticks & bars | Migrate to TimescaleDB/PostgreSQL for time-series optimization |
| **Resampling** | pandas | OHLCV aggregation | Use Apache Flink for streaming aggregation |
| **Analytics** | NumPy, statsmodels | Quant calculations | Implement in C++/Cython for performance |
| **Dashboard** | Streamlit, Plotly | Visualization | Use React + WebSocket for production UI |
| **Cache** | In-memory | Fast access | Add Redis for distributed caching |

---

## Analytics Explained

### 1. Hedge Ratio (Î²)

**OLS Regression**: `price_A = Î± + Î² Ã— price_B + Îµ`

- **Î² (beta)**: Number of units of B to hedge 1 unit of A
- **Calculation**: Ordinary Least Squares minimizes squared residuals
- **RÂ²**: Goodness of fit (higher is better, > 0.7 is good)

**Code**:
```python
beta, alpha, r_squared = analytics.calculate_hedge_ratio_ols(price_A, price_B)
```

### 2. Spread

**Definition**: `Spread = price_A - Î² Ã— price_B`

- Measures the price difference after hedging
- Stationary spread indicates cointegrated pairs
- Mean-reverting spread is tradeable

### 3. Z-Score

**Formula**: `Z = (Spread - Î¼_rolling) / Ïƒ_rolling`

- Standardized measure of spread deviation
- **Z > +2**: Spread is expensive (short spread)
- **Z < -2**: Spread is cheap (long spread)
- **|Z| < 1**: No trade signal

### 4. Rolling Correlation

**Pearson Correlation**: Measures linear relationship between price movements

- **Correlation > 0.7**: Strong positive relationship (good for pairs)
- **Rolling window**: Adapts to changing market conditions

### 5. ADF Test (Augmented Dickey-Fuller)

**Tests stationarity of spread**:
- **Null Hypothesis**: Spread has unit root (non-stationary)
- **p-value < 0.05**: Reject null â†’ Spread is stationary âœ…
- **Stationary spread**: Mean-reverting, suitable for pairs trading

### 6. Half-Life

**Mean reversion speed**:
- Ornstein-Uhlenbeck process parameter
- Lower half-life = faster mean reversion
- Typical range: 5-30 periods

---

## Quick Start

### Prerequisites

- Python 3.8+
- Internet connection (for WebSocket)

### Installation

```bash
# Clone or download the project
cd gemsap

# Install dependencies
pip install -r requirements.txt
```

### Running the Application

```bash
# Start the Streamlit dashboard
streamlit run app.py
```

The dashboard will open in your browser at `http://localhost:8501`

### Using the Dashboard

1. **Start Data Collection**:
   - Click "â–¶ï¸ Start" in the sidebar
   - System begins collecting live tick data

2. **Configure Parameters**:
   - Select symbols (e.g., btcusdt, ethusdt)
   - Choose timeframe (1s, 1m, 5m)
   - Adjust rolling window (20 is default)
   - Set z-score alert threshold

3. **Monitor Analytics**:
   - View real-time price charts
   - Monitor spread and z-score
   - Check correlation trends
   - Review statistics

4. **Run Tests**:
   - Click "ðŸ“Š Run ADF Test" for stationarity check
   - Interpret results (p-value < 0.05 is good)

5. **Export Data**:
   - Download spread & z-score as CSV
   - Export alert history

---

## Project Structure

```
gemsap/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_ingestion.py    # WebSocket collector & buffer
â”‚   â”œâ”€â”€ storage.py            # SQLite database layer
â”‚   â”œâ”€â”€ resampler.py          # OHLCV resampling engine
â”‚   â”œâ”€â”€ analytics.py          # Pairs trading analytics
â”‚   â””â”€â”€ pipeline.py           # Main orchestrator
â”œâ”€â”€ app.py                    # Streamlit dashboard
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ .gitignore
```

---

## ðŸ”§ Design Decisions

### Why SQLite?

**Pros**:
- Zero configuration
- File-based (portable)
- ACID transactions
- Good for single-machine deployment
- Fast for < 1M rows

**Cons**:
- Not suitable for distributed systems
- Limited concurrent writes

**Production Alternative**: TimescaleDB (PostgreSQL extension for time-series)

### Why Streamlit?

**Pros**:
- Pure Python (no JS required)
- Rapid development
- Built-in interactivity
- Good for internal tools

**Cons**:
- Slower than native web apps
- Limited customization

**Production Alternative**: React + FastAPI + WebSocket

### Why Pandas for Resampling?

**Pros**:
- Excellent time-series support
- `.resample()` is very concise
- Widely used in quant finance

**Cons**:
- Memory intensive for large datasets
- Not real-time streaming

**Production Alternative**: Apache Flink or kdb+ for tick-level streaming

### Why OLS for Hedge Ratio?

**Pros**:
- Simple, interpretable
- Fast to compute
- Industry standard

**Cons**:
- Assumes linear relationship
- Sensitive to outliers

**Advanced Alternatives**:
- Kalman Filter (dynamic hedge ratio)
- VECM (Vector Error Correction Model)
- Robust regression (handles outliers)

---

## Scaling Considerations

### Current Limitations

| Component | Current | Bottleneck | Solution |
|-----------|---------|------------|----------|
| Ingestion | Single thread | CPU for multiple symbols | Kafka + consumer group |
| Storage | SQLite | Concurrent writes | PostgreSQL/TimescaleDB |
| Resampling | Batch processing | Latency | Streaming aggregation (Flink) |
| Analytics | Synchronous | Blocking calculations | Async workers (Celery) |
| Dashboard | Streamlit | Refresh-based | WebSocket push updates |

### Production Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Market Data API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Cluster     â”‚ â† Distributed ingestion
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Flink â”‚  â”‚TimescaleDBâ”‚ â† Real-time + historical
â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Redis   â”‚ â† Low-latency cache
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FastAPI  â”‚ â† REST + WebSocket API
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  React   â”‚ â† Production UI
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Recommended Improvements

1. **Add Backtesting**: Simulate strategies on historical data
2. **Risk Management**: Position sizing, stop-loss logic
3. **Multiple Pairs**: Scan for cointegrated pairs automatically
4. **Machine Learning**: Predict spread movements with LSTM/XGBoost
5. **Order Execution**: Integration with exchange APIs
6. **Monitoring**: Prometheus + Grafana for system metrics
7. **Alerting**: Slack/Email notifications

---

## Testing

### Unit Tests
```bash
# (Not implemented yet - would use pytest)
pytest tests/
```

### Manual Testing Checklist

- [ ] WebSocket connects successfully
- [ ] Ticks are persisted to database
- [ ] Resampling produces valid OHLCV bars
- [ ] Hedge ratio calculation is reasonable
- [ ] Z-score alerts trigger correctly
- [ ] Data export works
- [ ] Dashboard refreshes properly

---

## Usage Notes

### ChatGPT Assistance

During the development of this project, I used ChatGPT as a collaborative learning tool to enhance my understanding and accelerate development. Here's how I leveraged it thoughtfully:

#### Learning & Conceptual Understanding
- Clarified complex statistical concepts like cointegration, ADF tests, and hedge ratios
- Explored different approaches to pairs trading strategies
- Understood the mathematical foundations behind z-score calculations

#### Technical Implementation Support
- Helped with boilerplate code structures for Streamlit layouts
- Provided examples of pandas resampling techniques
- Assisted with Plotly chart configurations for better visualization
- Guided on SQLite schema design patterns

#### Problem Solving
- Debugged tricky asynchronous programming issues with websockets
- Optimized performance bottlenecks in data processing pipelines
- Solved integration challenges between different components

#### Important Note
**All core analytics logic, architectural decisions, and quantitative methodologies were designed and implemented through my own understanding and expertise.** ChatGPT served as a valuable assistant for accelerating development and overcoming specific technical hurdles, but the fundamental intellectual contribution remains my own.

The use of AI was strategic - focusing on areas where I needed to bridge knowledge gaps or accelerate implementation, while preserving the integrity of the core quantitative and architectural work.

---

## Future Enhancements

### High Priority
- [ ] Automated pairs discovery (correlation + cointegration scan)
- [ ] Backtesting engine with PnL tracking
- [ ] Multiple timeframe analysis (MTF)
- [ ] Advanced hedge ratio (Kalman filter)

### Medium Priority
- [ ] Machine learning for spread prediction
- [ ] Risk metrics (Sharpe, max drawdown, VaR)
- [ ] Portfolio view (multiple pairs simultaneously)
- [ ] Historical data loader (backfill from CSV/Parquet)

### Nice to Have
- [ ] Docker containerization
- [ ] CI/CD pipeline
- [ ] Jupyter notebook examples
- [ ] API documentation (Swagger)

---

**Built with**: Python | Streamlit | Plotly | Pandas | NumPy | SQLite

**Purpose**: Quantitative developer assessment demonstrating end-to-end system design and analytics capabilities.

---

## Acknowledgments

- **Data Source**: Binance Futures API
- **Inspiration**: Statistical arbitrage research and classic pairs trading literature
- **Tools**: Open-source Python ecosystem

---
